services:
  mongo1:
    image: mongo:latest
    container_name: mongo1
    restart: unless-stopped
    ports:
      - 27017:27017
    command: ["mongod", "--replSet", "rs0", "--bind_ip_all"]
    volumes:
      - mongo1_data:/data/db

  mongo-setup:
    image: mongo:latest
    container_name: mongo-setup
    depends_on:
      - mongo1
    entrypoint: ["bash", "-c"]
    command: >
      "
      sleep 5 &&
      mongosh --host mongo1:27017 --eval '
        rs.initiate({
          _id: \"rs0\",
          members: [
            { _id: 0, host: \"mongo1:27017\" }
          ]
        })
      ' || true
      "

  postgres:
    image: postgres:16-alpine
    oom_score_adj: -900
    container_name: postgres
    restart: unless-stopped
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: postgres
    ports:
      - "5442:5432"
    volumes:
      - pgdata:/var/lib/postgresql/data
    command:
      [
        "postgres",
        "-c",
        "shared_buffers=128MB",
        "-c",
        "work_mem=4MB",
        "-c",
        "maintenance_work_mem=64MB",
        "-c",
        "max_connections=50",
        "-c",
        "wal_buffers=4MB",
        "-c",
        "effective_cache_size=256MB",
      ]
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Dedicated Postgres for agents persistence
  agents-db:
    image: postgres:16-alpine
    oom_score_adj: -900
    container_name: agents-db
    restart: unless-stopped
    environment:
      POSTGRES_USER: ${AGENTS_DB_USER:-agents}
      POSTGRES_PASSWORD: ${AGENTS_DB_PASSWORD:-agents}
      POSTGRES_DB: ${AGENTS_DB_NAME:-agents}
    ports:
      - "5443:5432"
    volumes:
      - agents_pgdata:/var/lib/postgresql/data
    command:
      [
        "postgres",
        "-c",
        "shared_buffers=128MB",
        "-c",
        "work_mem=4MB",
        "-c",
        "maintenance_work_mem=64MB",
        "-c",
        "max_connections=50",
        "-c",
        "wal_buffers=4MB",
        "-c",
        "effective_cache_size=256MB",
      ]
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${AGENTS_DB_USER:-agents} -d ${AGENTS_DB_NAME:-agents}"]
      interval: 10s
      timeout: 5s
      retries: 5

  vault:
    image: hashicorp/vault:1.17
    oom_score_adj: -900
    container_name: vault
    restart: unless-stopped
    cap_add:
      - IPC_LOCK
    ports:
      - "8200:8200"
    environment:
      VAULT_ADDR: http://127.0.0.1:8200
      VAULT_LOCAL_CONFIG: |
        ui = true
        cluster_name = "local-vault"
        disable_mlock = true
        listener "tcp" {
          address     = "0.0.0.0:8200"
          tls_disable = 1
        }
        storage "file" {
          path = "/vault/file"
        }
        api_addr     = "http://vault:8200"
        cluster_addr = "http://vault:8201"
    command: ["server"]
    volumes:
      - vault-file:/vault/file

  vault-auto-init:
    image: hashicorp/vault:1.17
    container_name: vault-auto-init
    depends_on:
      - vault
    restart: "no"
    environment:
      VAULT_ADDR: http://vault:8200
    volumes:
      - vault-file:/vault/file
      - ./vault/auto-init.sh:/auto-init.sh:ro
    entrypoint: ["/bin/sh", "/auto-init.sh"]
  # Lightweight Docker registry mirror (proxy cache)
  # - Uses official registry:2 with proxy enabled via env var
  # - Exposed only on the user-defined bridge network 'agents_net' (no host ports)
  # - DinD sidecars in workspaces will use http://registry-mirror:5000 as --registry-mirror
  registry-mirror:
    image: registry:2
    restart: unless-stopped
    environment:
      REGISTRY_HTTP_ADDR: 0.0.0.0:5000
      REGISTRY_PROXY_REMOTEURL: https://registry-1.docker.io
    networks:
      - agents_net

  # Nix Content-Addressable Proxy Server (ncps)
  # Provides a nix binary cache proxy for workspace containers.
  # - No host ports exposed; reachable on agents_net as http://ncps:8501
  # - Persistent storage backed by the ncps-storage volume
  ncps-init:
    image: alpine:3.20
    command:
      [
        "/bin/sh",
        "-c",
        "mkdir -m 0755 -p /storage/var && mkdir -m 0700 -p /storage/var/ncps && mkdir -m 0700 -p /storage/var/ncps/db",
      ]
    volumes:
      - ncps-storage:/storage
    restart: "no"
    networks:
      - agents_net

  ncps-migrate:
    image: kalbasit/ncps:latest
    depends_on:
      ncps-init:
        condition: service_completed_successfully
    # Run dbmate schema migrations. Note: 'dbmate up' is a one-time schema init per storage volume.
    command:
      ["/bin/dbmate", "--url=sqlite:/storage/var/ncps/db/db.sqlite", "up"]
    volumes:
      - ncps-storage:/storage
    restart: "no"
    networks:
      - agents_net

  ncps:
    image: kalbasit/ncps:latest
    oom_score_adj: -900
    depends_on:
      ncps-migrate:
        condition: service_completed_successfully
    command:
      - /bin/ncps
      - serve
      - --server-addr=0.0.0.0:8501
      - --cache-hostname=ncps
      - --cache-data-path=/storage
      - --cache-database-url=sqlite:/storage/var/ncps/db/db.sqlite
      - --upstream-cache=https://cache.nixos.org
      - --upstream-public-key=cache.nixos.org-1:6NCHdD59X431o0gWypbMrAURkbJ16ZPMQFGspcDShjY=
    ports:
      - "8501:8501"
    volumes:
      - ncps-storage:/storage
    networks:
      - agents_net
    environment:
      - PROMETHEUS_ENABLED=true
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "-qO-", "http://localhost:8501/nix-cache-info"]
      interval: 30s
      timeout: 3s
      retries: 5
      start_period: 20s

  # LiteLLM Postgres database
  litellm-db:
    image: postgres:16-alpine
    oom_score_adj: -900
    restart: unless-stopped
    environment:
      POSTGRES_DB: litellm
      POSTGRES_USER: litellm
      POSTGRES_PASSWORD: change-me
    volumes:
      - litellm_pgdata:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U litellm -d litellm"]
      interval: 10s
      timeout: 5s
      retries: 5
      # Allow time for initial database startup & init
      start_period: 20s
    networks:
      - agents_net

  # LiteLLM server and UI
  litellm:
    image: ghcr.io/berriai/litellm:v1.77.5-stable
    oom_score_adj: -900
    restart: unless-stopped
    environment:
      LITELLM_MASTER_KEY: ${LITELLM_MASTER_KEY:-sk-dev-master-1234}
      LITELLM_SALT_KEY: ${LITELLM_SALT_KEY:-sk-dev-salt-1234}
      DATABASE_URL: postgresql://litellm:change-me@litellm-db:5432/litellm
      STORE_MODEL_IN_DB: "True"
      UI_USERNAME: ${LITELLM_UI_USERNAME:-admin}
      UI_PASSWORD: ${LITELLM_UI_PASSWORD:-admin}
      PORT: "4000"
      HOST: "0.0.0.0"
    depends_on:
      litellm-db:
        condition: service_healthy
    ports:
      # Bind to loopback in dev to avoid exposing externally.
      # Change to "0.0.0.0:4000:4000" or "4000:4000" to expose on LAN.
      - "127.0.0.1:4000:4000"
    healthcheck:
      # Prefer HTTP health endpoint; fall back to UI or root if /health absent.
      # If wget/curl are unavailable in the image, fall back to a TCP check via nc.
      test:
        [
          "CMD-SHELL",
          "wget -qO- http://localhost:4000/health || wget -qO- http://localhost:4000/ui || wget -qO- http://localhost:4000/ || (command -v curl >/dev/null 2>&1 && curl -fsS http://localhost:4000/health) || (command -v nc >/dev/null 2>&1 && nc -z localhost 4000)",
        ]
      interval: 15s
      timeout: 3s
      retries: 5
      start_period: 10s
    networks:
      - agents_net

  cadvisor:
    image: gcr.io/cadvisor/cadvisor:latest
    container_name: cadvisor
    restart: unless-stopped
    ports:
      - "8080:8080"
    volumes:
      - type: bind
        source: /var/run/docker.sock
        target: /var/run/docker.sock
        read_only: true
      - type: bind
        source: /sys
        target: /sys
        read_only: true
      - type: bind
        source: /var/lib/docker
        target: /var/lib/docker
        read_only: true
    networks:
      - agents_net

  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    restart: unless-stopped
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
      - "--storage.tsdb.retention.time=7d"
    depends_on:
      - cadvisor
    ports:
      - "9090:9090"
    volumes:
      - type: bind
        source: ./monitoring/prometheus
        target: /etc/prometheus
        read_only: true
      - prometheus-data:/prometheus
    networks:
      - agents_net

  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    restart: unless-stopped
    depends_on:
      - prometheus
    ports:
      - "3000:3000"
    volumes:
      - type: bind
        source: ./monitoring/grafana/provisioning
        target: /etc/grafana/provisioning
        read_only: true
      - grafana-data:/var/lib/grafana
    networks:
      - agents_net

volumes:
  mongo1_data:
    driver: local
  vault-file:
    driver: local
  pgdata:
  ncps-storage:
    driver: local
  litellm_pgdata:
    driver: local
  agents_pgdata:
    driver: local
  prometheus-data:
    driver: local
  grafana-data:
    driver: local

networks:
  # Shared user-defined bridge with deterministic name so non-compose containers
  # can attach to it (e.g., workspace containers launched via Docker API).
  agents_net:
    name: agents_net
